{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/psypl-experiments/.env/lib/python3.7/site-packages/rpy2/robjects/vectors.py:969: UserWarning: R object inheriting from \"POSIXct\" but without attribute \"tzone\".\n",
      "  warnings.warn('R object inheriting from \"POSIXct\" but without '\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from psypl.experiments import (\n",
    "    VariableCuedRecallExperiment, VariableArithmeticSequenceExperiment, FunctionAlignExperiment,\n",
    "    FunctionBasicExperiment, FunctionDepthExperiment, SemanticNamesExperiment, VariableDistanceExperiment,\n",
    "    VariableCountExperiment, FunctionMemoryExperiment)\n",
    "from pickle_cache import PickleCache\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pymer4 import Lmer, Lm\n",
    "from scipy.stats import shapiro\n",
    "import warnings\n",
    "from pickle_cache import PickleCache\n",
    "\n",
    "pcache = PickleCache()\n",
    "warnings.simplefilter(\"ignore\")\n",
    "sns.set(rc={'figure.figsize':(10, 5)})\n",
    "\n",
    "def remove_outliers(df, columns, measure='duration', err=0.05):\n",
    "    bounds = df.groupby(columns)[measure].quantile([err, 1-err]).unstack(level=[len(columns)]).reset_index()\n",
    "    df = pd.merge(df, bounds, on=columns)\n",
    "    return df[(df[measure] > df[err]) & (df[measure] < df[1-err])] \n",
    "\n",
    "def remove_incorrect_participants(df, threshold=0.5):\n",
    "    mean_correct = df.groupby('participant').mean().correct\n",
    "    bad_participants = mean_correct[mean_correct < threshold].index.tolist()\n",
    "    print(f'Removing {len(bad_participants)} participants for poor performance')\n",
    "    return df[~df.participant.isin(bad_participants)]\n",
    "\n",
    "def normality_test(df, dv='duration', group=['participant', 'cond']):\n",
    "    # Run Shapiro's test on each group\n",
    "    pvalue = df.groupby(group).apply(lambda df2: shapiro(df2[dv])[1]).rename('pvalue')\n",
    "    \n",
    "    # Apply Bonferroni's correction\n",
    "    threshold = 0.05 / len(pvalue) \n",
    "    significant = pvalue.apply(lambda p: 1 if p < threshold else 0).rename('significant')\n",
    "    \n",
    "    newdf = pd.concat([pvalue, significant], axis=1).reset_index()\n",
    "    return newdf[['pvalue', 'significant']].mean()#.describe()[[('pvalue', 'mean'), ('significant', 'mean')]]\n",
    "\n",
    "def contrast_stats(pairwise, contrast):\n",
    "    row = pairwise[pairwise.Contrast == contrast].iloc[0]\n",
    "    mean = np.exp(-row.Estimate)\n",
    "    std = mean * np.exp(row.SE) - mean\n",
    "    return mean, std\n",
    "   \n",
    "client = MongoClient('mongodb://moc:moc@localhost:27017/experiments?authSource=admin')\n",
    "experiments_db = client.experiments.experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "exp1 = VariableCuedRecallExperiment()\n",
    "exp1_results = exp1.get_mongo_results(experiments_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Variable cued recall\n",
    "\n",
    "This experiment tests basic memory capacity for pairs of variables and values. The participant is presented a sequence of $N_{var} \\in \\{4, 7\\}$ pairs of the form `x = 1`. Each pair is shown for 2 seconds. Immmediately after the final pair, the participant is shown the variables in random order (e.g. `y = ?; x = ?; ...`) and asked to fill in the remembered value.\n",
    "\n",
    "We test three conditions: single letter names (e.g. `r`, `q`), nonsense syllables (e.g. `jux`, `yip`), and common English nouns (e.g. `cave`, `tax`). The current experiment includes {{len(exp1_results.participant.unique())}} participants with {{len(exp1_results) // len(exp1_results.participant.unique())}} trials per participant. \n",
    "\n",
    "In this and all experiments using within-subjects conditions, the experiments are done using a factorial design (e.g. 2 values of $N$ by 3 conditions = 6 pairs). Trials are counterbalanced (randomized) for each participant.\n",
    "\n",
    "For each trial, we measure the number of correctly remembered variable/value pairs. The hypotheses:\n",
    "* Participants should remember a smaller fraction of variables at $N = 7$ than $N = 4$ due to working memory capacity.\n",
    "* Letters and syllables are harder to remember than words due to a lack of semantic meaning.\n",
    "\n",
    "Below we plot the overall distribution for each independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(data=exp1_results, x='N_var', y='correct_frac', hue='cond', cut=0)\n",
    "ax.set_title('Variable cued recall: accuracy distribution')\n",
    "ax.set_ylabel('Fraction of correct responses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* The $N = 7$ condition does not seem substantively worse than $N = 4$, indicating our first hypothesis may be incorrect.\n",
    "* The word condition seems to be slightly worse than the other two conditions, indicating our second hypothesis is incorrect.\n",
    "\n",
    "We can confirm these suspicions by running an ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_model = Lmer('correct_frac ~ cond * N_var + (1 | participant)', data=exp1_results)\n",
    "exp1_model.fit(factors={\n",
    "    'cond': list(map(str, exp1.Condition)), \n",
    "    'N_var': [4, 7],\n",
    "}, summarize=False)\n",
    "exp1_model.anova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, neither the condition nor $N$ have a statistically significant relationship with the fraction of correctly recalled variables. We can dig a bit deeper by looking at the pairwise relationships of conditions for each N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pairwise = exp1_model.post_hoc(marginal_vars='cond', grouping_vars='N_var')\n",
    "pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Letter vs. Word condition for $N = 7$ is the closest to significant, but still far from it.\n",
    "\n",
    "**Conclusion:** this experiment should be re-run with larger values of $N$ to find the point at which memory falls off. Alternatively, we can consider doing an interference task between memorization and recall (similar to other cued recall studies of paired-associate learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "exp2 = VariableArithmeticSequenceExperiment()\n",
    "exp2_results = exp2.get_mongo_results(experiments_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Variable arithmetic sequence \n",
    "\n",
    "This experiment tests how a person can remember variable/value pairs while repeatedly performing an interfering task (basic arithmetic). The participant is presented with a sequence of arithmetic expressions like `x = 1; y = 2 - x; ...` that use past variables from the current sequence. There is a 2 second delay between stages of the sequence to enforce a minimum time delay. The sequence continues until the participant gives an incorrect answer. We measure the length of the sequence before stopping.\n",
    "\n",
    "The basic hypothesis is that the arithmetic causes interference which should reduce memory capacity compared to the first experiment. The mean stage of failure should be less than the mean immediate recall capacity for variables. Based on a similar experiment in Campbell and Charness '91, we would expect the number of errors to peak by stage 4.\n",
    "\n",
    "Below, we plot the distribution of stages of failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(0, exp2_results.stage.max() + 1))\n",
    "ax = sns.distplot(exp2_results.stage, kde=False, bins=bins)\n",
    "ax.set_xlabel('Stage of failure')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticks(np.arange(0.5, exp2_results.stage.max() + 0.5))\n",
    "ax.set_xticklabels(bins)\n",
    "\n",
    "plt.title('Arithmetic sequence experiment: stage of failure distribution', y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is drastically worse compared to the prior experiment. Participants can barely remember 2 variables, let alone 7! The median stage of failure is {{int(exp2_results.median().stage)}}, and the mean is {{exp2_results.mean().stage}}.\n",
    "\n",
    "**Conclusion**: the hypothesis is supported by the data, even moreso than predicted by comparison to Campbell and Charness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "exp3 = FunctionBasicExperiment()\n",
    "exp3_results = exp3.get_mongo_results(experiments_db)\n",
    "exp3_results = remove_outliers(exp3_results, ['participant', 'cond'])\n",
    "exp3_results = remove_incorrect_participants(exp3_results)\n",
    "exp3_results['log_duration'] = np.log(exp3_results.duration)\n",
    "exp3_col_order = [str(c).split('.')[1] for c in exp3.Condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Straight-line code vs. functions\n",
    "\n",
    "Next, we consider comparing a sequence of arithmetic statements vs. an equivalent sequence of function calls. In this and the remaining experiments, rather than presenting programs one line at a time, we instead present the entire program at once, and ask the participant to trace its output. Subsequently, we move from direct measures of memory to measuring response time, with the general hypothesis that increasing load on working memory will increase response times.\n",
    "\n",
    "Here, we consider three conditions. First, `NoFunction` which is a sequence of straight line arithmetic expressions, e.g.\n",
    "\n",
    "\n",
    "```python\n",
    "x = 1 \n",
    "y = x - 4\n",
    "z = y - x\n",
    "```\n",
    "\n",
    "Next, we consider `SimpleFunction` which moves the arithmetic expressions into standalone functions. For example, the program above would be rewritten as:\n",
    "\n",
    "```python\n",
    "def f():\n",
    "    return 1\n",
    "def q(x):\n",
    "    return x - 4\n",
    "def w(y, x):\n",
    "    return y - x\n",
    "\n",
    "x = f()\n",
    "y = q(x)\n",
    "z = w(y, x)\n",
    "```\n",
    "\n",
    "Finally, we add a `RenameArgsFunction` condition that randomly changes the function definition parameter names, e.g.\n",
    "\n",
    "\n",
    "```python\n",
    "def f():\n",
    "    return 1\n",
    "def q(a):\n",
    "    return a - 4\n",
    "def w(u, r):\n",
    "    return u - r\n",
    "\n",
    "x = f()\n",
    "y = q(x)\n",
    "z = w(y, x)\n",
    "```\n",
    "\n",
    "The basic hypotheses:\n",
    "* Tracing functions should take more time  than tracing equivalent straight-line code because of the time required to find the function and locate the body. \n",
    "* This cognitive load likely induces forgetting of variables held in working memory.\n",
    "* A function with arguments named the same as the inputs (`SimpleFunction`) should take less time to trace than one with random names (`RenameArgs`) because the participant does not have to hold the source <-> destination variable mappings in their head.\n",
    "\n",
    "This study was run with {{len(exp3_results.participant.unique())}} participants (some participants were excluded if they answered less than 50% of the trials correctly). They were evaluated on randomly generated programs containing $N_{var} = 6$ variables in each condition. Below, I plot the distribution of response times for each condition, separating correct from incorrect responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = exp3_results.groupby('cond').median().duration\n",
    "sns.violinplot(\n",
    "    data=exp3_results, \n",
    "    x='cond', y='duration', hue='correct', \n",
    "    hue_order=[0,1], order=exp3_col_order, cut=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* There seem to be a number of extremely short responses that were incorrect, indicating guessing or laziness.\n",
    "* The function conditions to indeed seem to take longer than the straight-line condition (NoFunction median {{f'{medians[\"NoFunction\"]:.02f}'}}, SimpleFunction median {{f'{medians[\"SimpleFunction\"]:.02f}'}}). The relationship between SimpleFunction and RenameArgs is less clear.\n",
    "\n",
    "It doesn't seem like we can directly perform statistical tests due to its clear right skew. We can check the average normality by using a Shapiro-Wilk test on each participant's per-condition distribution, and seeing how many are possibly normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_test(exp3_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 5% of the tested distributions should be considered normal according to the Shapiro-Wilk test. While that isn't a large number, we can still increase normality by using a log distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_test(exp3_results, dv='log_duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, running an ANOVA over a mixed-effects model with the condition and correctness as fixed effects and participant as a random effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_model = Lmer('log_duration ~ cond + correct + (1 | participant)', data=exp3_results)\n",
    "exp3_model.fit(factors={'cond': exp3_col_order, 'correct': [0, 1]}, summarize=False)\n",
    "exp3_model.anova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is significant variation within both the conditions and between correct and incorrect responses. We can dive into the differences between conditions using pair-wise post hoc tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pairwise = exp3_model.post_hoc(marginal_vars='cond')\n",
    "\n",
    "est1_mean, est1_std = contrast_stats(pairwise, 'NoFunction - SimpleFunction')\n",
    "est2_mean, est2_std = contrast_stats(pairwise, 'NoFunction - RenameArgsFunction')\n",
    "\n",
    "pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "The `NoFunction` condition is statistically significantly different from both the `SimpleFunction` and `RenameArgsFunction` conditions. `NoFunction` is an estimated {{f'{est1_mean:.02f}'}}x +/- {{f'{est1_std:.02f}'}} faster to trace than `SimpleFunction`, and {{f'{est2_mean:.02f}'}}x +/- {{f'{est2_std:.02f}'}} than `RenameArgs`.\n",
    "\n",
    "`RenameArgsFunction` is on average slower to trace than `SimpleFunction`, but the relationship is not statistically significant with the current number of participants and trials.\n",
    "\n",
    "**Conclusion**: the hypothesis that straight-line code is easier to trace than equivalent function calls is supported by the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Aligning positional arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "exp4 = FunctionAlignExperiment()\n",
    "exp4_results = exp4.get_mongo_results(experiments_db)\n",
    "\n",
    "# Only keep N == 6 results for now\n",
    "exp4_results = exp4_results[exp4_results.N_var == 6]\n",
    "\n",
    "# Eliminate outliers (more than 5 minutes on an answer)\n",
    "exp4_results = remove_outliers(exp4_results, ['participant'])\n",
    "\n",
    "# Remove participants who don't get at least 50% of their answers right\n",
    "exp4_results = remove_incorrect_participants(exp4_results)\n",
    "\n",
    "exp4_results['log_duration'] = np.log(exp4_results.duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While the prior experiment did not reveal effects in the choice of names for function definition parameters, another potential source of cognitive load comes from aligning call-site arguments to definition-site parameters. For example, consider these two equivalent programs:\n",
    "\n",
    "Program 1 (Misaligned):\n",
    "\n",
    "```python\n",
    "def e(a, n, w, x):\n",
    "    return x - n - a - w\n",
    "\n",
    "e(4, 6, 6, 9)\n",
    "```\n",
    "\n",
    "Program 2 (Aligned):\n",
    "\n",
    "```python\n",
    "    e(4, 6, 6, 9)\n",
    "def e(a, n, w, x):\n",
    "    return x - n - a - w\n",
    "```\n",
    "\n",
    "While program 1 is in the standard format of a program, I hypothesize that program 2 would be easier to trace because it's perceptually simpler to line up the arguments and parameters. \n",
    "\n",
    "To test this hypothesis, I randomly generated programs of the above two forms with 6 parameters and asked participants to compute the output. Below is the distribution of response times for each condition, separated by incorrect (blue) and correct (orange) responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=exp4_results, x='cond', y='duration', hue='correct', cut=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Interestingly, unlike the prior experiment, incorrect answers seemed to take *more* time than correct answers. Possibly because incorrect answers were given for individually harder problems?\n",
    "* The misaligned condition (median {{f\"{exp4_results[exp4_results.cond == 'Condition.Misaligned'].duration.median():.02f}s\"}}) does seem to take more time than the aligned condition (median {{f\"{exp4_results[exp4_results.cond == 'Condition.Aligned'].duration.median():.02f}s\"}}). \n",
    "\n",
    "We can confirm with statistical tests. As before, using log duration to normalize the right-tail distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4_model = Lmer('log_duration ~ cond + correct + (1 | participant)', data=exp4_results)\n",
    "exp4_model.fit(factors={'cond': list(map(str, exp4.Condition))}, summarize=False)\n",
    "\n",
    "coef = exp4_model.coefs.loc['condCondition.Misaligned']\n",
    "ratio_mean = np.exp(coef.Estimate)\n",
    "ratio_std = ratio_mean * (np.exp(coef.SE) - 1)\n",
    "\n",
    "exp4_model.anova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a statistically significant difference between the conditions. We can use the regression coefficient to determine magnitude of difference: the misaligned condition is an estimated {{f\"{ratio_mean:.02f}\"}}x +/- {{f\"{ratio_std:.02f}\"}} slower than the aligned condition.\n",
    "\n",
    "**Conclusion:** the hypothesis that misalignment of positional arguments is more challenging to trace is supported by the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: Straight-line code vs. nested functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "exp5 = FunctionDepthExperiment()\n",
    "exp5_results = exp5.get_mongo_results(experiments_db)\n",
    "exp5_results = remove_outliers(exp5_results, ['participant', 'cond'])\n",
    "exp5_results = remove_incorrect_participants(exp5_results)\n",
    "exp5_results['log_duration'] = np.log(exp5_results.duration)\n",
    "exp5_col_order = list(map(str, exp5.Condition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the prior straightline vs. function experiment, each expression was pulled into a separate function, but the control flow was still structured/regular in the sense of going back and forth between statements/functions with a clear pattern. In this experiment, we want to evaluate the effect of having functions jump out of order to different functions.\n",
    "\n",
    "Specifically, we consider three different kinds of programs. One without variables or functions, one with straight-line variables, and one with non-linear function calls.\n",
    "\n",
    "Program 1 `Parentheses`:\n",
    "\n",
    "```python\n",
    "(3 - 2) - ((4 - 1) + (3 - 6))\n",
    "```\n",
    "\n",
    "Program 2 `Variable`:\n",
    "\n",
    "```python\n",
    "x = 4 - 1\n",
    "y = 3 - 6\n",
    "z = x + y\n",
    "w = 3 - 2\n",
    "q = w - z\n",
    "q\n",
    "```\n",
    "\n",
    "Program 3 `Preorder`:\n",
    "\n",
    "```python\n",
    "def a():\n",
    "    return 4 - 1\n",
    "def b():\n",
    "    return 3 - 6\n",
    "def c():\n",
    "    return a() + b()\n",
    "def d():\n",
    "    return 3 - 2\n",
    "def e():\n",
    "    return d() - c()\n",
    "e()\n",
    "```\n",
    "\n",
    "I hypothesize that Program 3 should be harder than Program 2, which should be harder than Program 1. \n",
    "* For Program 3, the participant has to remember both where they are in the computation (the stack of function return pointers, essentially) along with the actual intermediate values.\n",
    "* For program 2, the participant just has to remember the variable/value bindings as they proceed linearly through the program. \n",
    "* For program 1, the participant only has to remember a single intermediate while they search for the next operation to perform.\n",
    "\n",
    "As before, we ask participants to compute the output of the program for $N_{var} = 6$ in each of the conditions, measuring the response time. The distribution of response times is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = exp5_results.groupby('cond').median().duration\n",
    "sns.violinplot(data=exp5_results, x='cond', y='duration', hue='correct', order=exp5_col_order, cut=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* There doesn't seem to be a big difference between correct and incorrect responses.\n",
    "* The conditions do seem to get progressively harder as predicted. Parentheses has a median {{f\"{medians['Condition.Parentheses']:.02f}s\"}}, Variable has a median {{f\"{medians['Condition.Variable']:.02f}s\"}}, and Preorder has a median {{f\"{medians['Condition.Preorder']:.02f}s\"}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp5_model = Lmer('log_duration ~ cond + correct + (1 | participant)', data=exp5_results)\n",
    "exp5_model.fit(factors={'cond': exp5_col_order}, summarize=False)\n",
    "exp5_model.anova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditions are statistically significantly different. We then do a pairwise posthoc comparison to contrast the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pairwise = exp5_model.post_hoc(marginal_vars='cond')\n",
    "\n",
    "est1_mean, est1_std = contrast_stats(pairwise, 'Condition.Parentheses - Condition.Variable')\n",
    "est2_mean, est2_std = contrast_stats(pairwise, 'Condition.Parentheses - Condition.Preorder')\n",
    "est3_mean, est3_std = contrast_stats(pairwise, 'Condition.Variable - Condition.Preorder')\n",
    "\n",
    "pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pairwise differences are statistically sigificant. The estimated differences:\n",
    "* Variable is {{f\"{est1_mean:.02f}\"}}x +/- {{f\"{est1_std:.02f}\"}} slower than Parentheses.\n",
    "* Preorder is {{f\"{est2_mean:.02f}\"}}x +/- {{f\"{est2_std:.02f}\"}} slower than Parentheses.\n",
    "* Preorder is {{f\"{est3_mean:.02f}\"}}x +/- {{f\"{est3_std:.02f}\"}} slower than Variable.\n",
    "\n",
    "**Conclusion:** the hypotheses that the variable programs are harder than variable-less programs, and the function programs are harder than the variable programs, are both supported by the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: Semantic names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp6 = SemanticNamesExperiment()\n",
    "exp6_results = exp6.get_mongo_results(experiments_db)\n",
    "exp6_results = remove_incorrect_participants(exp6_results)\n",
    "exp6_order = list(map(str, exp6.Condition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=exp6_results, x='cond', y='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp6_results.groupby('cond').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp6_model = Lmer('duration ~ cond + (1 | participant) + (1 | func)', \n",
    "                  data=exp6_results.rename(columns={'function': 'func'}))\n",
    "exp6_model.fit(factors={\n",
    "    'cond': exp6_order,\n",
    "    'func': exp6_results.function.unique()\n",
    "})\n",
    "exp6_model.anova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp7 = VariableDistanceExperiment()\n",
    "exp7_results = pd.concat([exp7.get_mongo_results(experiments_db), pcache.get('exp6_pilot')])\n",
    "exp7_results = exp7_results[exp7_results.participant != 'mturk-A182FTWJLWJP1E']\n",
    "exp7_order = list(map(str, exp7.Condition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(data=exp7_results, x='cond', y='duration', order=exp7_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp7_model = Lmer('duration ~ cond + experience + (1 | participant)', data=exp7_results)\n",
    "exp7_model.fit(factors={\n",
    "    'cond': exp7_order,\n",
    "    'experience': ['<1', '1-3', '3-5', '5+']\n",
    "})\n",
    "exp7_model.anova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp8 = VariableCountExperiment()\n",
    "exp8_results = exp8.get_mongo_results(experiments_db)\n",
    "exp8_results = remove_incorrect_participants(exp8_results, threshold=0.3)\n",
    "#exp7_results = pd.concat([exp7.get_mongo_results(experiments_db), pcache.get('exp6_pilot')])\n",
    "#exp7_results = exp7_results[exp7_results.participant != 'mturk-A182FTWJLWJP1E']\n",
    "exp8_order = list(map(str, exp8.Condition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=exp8_results[exp8_results.correct == 1], x='N_op', y='duration', hue='N_var', cut=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=exp8_results[(exp8_results.N_var > 0) & (exp8_results.correct == 1)], x='N_op', y='duration', hue='cond', cut=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp8_model = Lmer('log(duration) ~ cond * N_var + (1 | N_op) + (1 | participant)', \n",
    "                  data=exp8_results[(exp8_results.N_var > 0) & (exp8_results.correct == 1)][['cond', 'N_var', 'N_op', 'participant', 'duration']])\n",
    "exp8_model.fit(factors={\n",
    "    'cond': exp8_order,\n",
    "    'N_op': [6, 9],\n",
    "    'N_var': [0, 2, 4]\n",
    "})\n",
    "exp8_model.anova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 participants for poor performance\n"
     ]
    }
   ],
   "source": [
    "exp9 = FunctionMemoryExperiment()\n",
    "exp9_results = exp9.get_mongo_results(experiments_db)\n",
    "exp9_results = remove_incorrect_participants(exp9_results, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 2, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 4, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 2}, {'depth': 2, 'visit': 2}, {'depth': 3, 'visit': 3}, {'depth': 4, 'visit': 3}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 2}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 2, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 2}, {'depth': 4, 'visit': 1}, {'depth': 5, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 4, 'visit': 1}, {'depth': 5, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 4, 'visit': 1}, {'depth': 5, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 2}, {'depth': 2, 'visit': 1}, {'depth': 2, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 2}, {'depth': 3, 'visit': 1}, {'depth': 4, 'visit': 2}, {'depth': 4, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 2}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 4, 'visit': 1}, {'depth': 4, 'visit': 1}]\n",
      "err\n",
      "err\n",
      "err\n",
      "err\n",
      "err\n",
      "err\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 2, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 2}, {'depth': 3, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 4, 'visit': 1}, {'depth': 4, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 2}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 4, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n",
      "[{'depth': 1, 'visit': 1}, {'depth': 2, 'visit': 1}, {'depth': 3, 'visit': 1}]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_depth(functions, f, d):\n",
    "    all_depths = {f: d}\n",
    "    for ref in functions[f]['refs']:\n",
    "        depths = compute_depth(functions, ref['name'], d+1)\n",
    "        all_depths = {**all_depths, **depths}\n",
    "    return all_depths\n",
    "\n",
    "for _, row in exp9_results.iterrows():\n",
    "    depths = compute_depth(row.functions, 'main', 0)\n",
    "    counts = defaultdict(int)\n",
    "    for entry in row.telemetry:\n",
    "        if entry['action'] == 'enter':\n",
    "            counts[entry['target']] += 1\n",
    "    try:\n",
    "        print([\n",
    "            {'depth': depths[k], 'visit': v}\n",
    "            for k,v in counts.items()\n",
    "        ])\n",
    "    except KeyError:\n",
    "        print('err')\n",
    "    #print(dict(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'main': {'refs': [{'end': 3, 'name': 'w', 'st...\n",
       "1     {'g': {'refs': [{'end': 27, 'name': 'k', 'star...\n",
       "2     {'a': {'refs': [{'end': 27, 'name': 'h', 'star...\n",
       "3     {'g': {'refs': [], 'source': 'def g():\n",
       "    ret...\n",
       "4     {'a': {'refs': [], 'source': 'def a():\n",
       "    ret...\n",
       "5     {'f': {'refs': [{'end': 23, 'name': 'v', 'star...\n",
       "6     {'e': {'refs': [], 'source': 'def e():\n",
       "    ret...\n",
       "7     {'b': {'refs': [], 'source': 'def b():\n",
       "    ret...\n",
       "8     {'d': {'refs': [], 'source': 'def d():\n",
       "    ret...\n",
       "9     {'c': {'refs': [], 'source': 'def c():\n",
       "    ret...\n",
       "20    {'c': {'refs': [], 'source': 'def c():\n",
       "    ret...\n",
       "21    {'a': {'refs': [{'end': 27, 'name': 'x', 'star...\n",
       "22    {'c': {'refs': [], 'source': 'def c():\n",
       "    ret...\n",
       "23    {'f': {'refs': [{'end': 23, 'name': 'u', 'star...\n",
       "24    {'g': {'refs': [{'end': 23, 'name': 'z', 'star...\n",
       "25    {'f': {'refs': [], 'source': 'def f():\n",
       "    ret...\n",
       "26    {'m': {'refs': [], 'source': 'def m():\n",
       "    ret...\n",
       "27    {'main': {'refs': [{'end': 3, 'name': 'n', 'st...\n",
       "28    {'e': {'refs': [{'end': 23, 'name': 'r', 'star...\n",
       "29    {'j': {'refs': [], 'source': 'def j():\n",
       "    ret...\n",
       "30    {'a': {'refs': [{'end': 23, 'name': 'd', 'star...\n",
       "31    {'c': {'refs': [{'end': 27, 'name': 'q', 'star...\n",
       "32    {'a': {'refs': [], 'source': 'def a():\n",
       "    ret...\n",
       "33    {'b': {'refs': [], 'source': 'def b():\n",
       "    ret...\n",
       "34    {'d': {'refs': [{'end': 23, 'name': 'v', 'star...\n",
       "35    {'b': {'refs': [], 'source': 'def b():\n",
       "    ret...\n",
       "36    {'g': {'refs': [{'end': 27, 'name': 'r', 'star...\n",
       "37    {'a': {'refs': [{'end': 27, 'name': 'x', 'star...\n",
       "38    {'e': {'refs': [], 'source': 'def e():\n",
       "    ret...\n",
       "39    {'c': {'refs': [{'end': 23, 'name': 'h', 'star...\n",
       "Name: functions, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp9_results.functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
